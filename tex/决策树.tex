\chapter{决策树}

\section{决策树的基本流程}\label{sec:6.1}
决策树的基本策略是：\textbf{“分而治之”}。

决策树从根节点开始，不断地寻找“划分”属性，将数据集递归地不断划分为小集合。可以说，决策树是一个从根到叶的递归过程，每个中间节点对应一个属性测试，该递归的终止条件为：

\begin{itemize}
    \item 当前节点包含的样本已经全部属于同一类别，\textbf{无需划分}；
    \item 当前\textbf{属性集为空}，或所有样本在所有属性上取值相同，\textbf{无法划分}；这种情况下，生成一个叶节点，类别为该节点所含样本最多的类别。
    \item 当前节点的\textbf{样本集为空，不能划分}。这种情况下，生成一个叶节点，类别为父节点所含样本最多的类别。
\end{itemize}
\section{划分选择}\label{sec:6.2}
决策树学习的关键在于\textbf{选择最优的划分标准}。我们希望决策树的分支节点所包含的样本尽可能属于同一类别，即\textbf{“纯度”}越高越好。

\subsection{信息增益}
信息增益的划分选择依赖于信息熵，信息熵(information entropy) 是度量样本集合纯度最常用的一种指标。假定当前样本集合$D$中第$k$类样本所占的比例为$p_k (k = 1,2,. . . , |\mathcal{Y}|)$，则D的信息熵定义为：
\[
\operatorname{Ent}(D)=-\sum_{k=1}^{\mid \mathcal{Y |}} p_{k} \log _{2} p_{k}
\]
$\operatorname{Ent}(D)$值越小，$D$的纯度越高。

假设某个属性$a$有V个可能的取值$\{a^1,a^2,...,a^V\}$，在决策树上就会产生V个分支节点，第v个节点包括了所有在a上取值为$a^v$的样本，记作$D^v$，我们可以计算出节点的信息熵，并且为其乘上一个系数，该系数由该取值对应的样本数决定，被称之为“信息增益”。
\[
\operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)
\]
\marginpar{\footnotesize 信息增益偏好取值数多的属性。}
信息增益越大，用属性a进行划分得到的纯度提升越大。

\subsection{增益率}
增益率为平衡属性偏好的不利影响，引入“增益率”选择最优划分属性。
\marginpar{\footnotesize 增益率偏好取值数较少的属性。}
\[
\operatorname{Gain\_ ratio}(D, a)=\frac{\operatorname{Gain}(D, a)}{\operatorname{IV}(a)} = \frac{\operatorname{Gain}(D,a)}{-\sum_{v=1}^{V}\frac{\left|D^{v}\right|}{|D|} \log _{2} \frac{\left|D^{v}\right|}{|D|}}
\]

\section{剪枝处理}\label{sec:6.3}
决策树的决策分支过多，可能导致将训练集自身的一些特性当作所有数据均具有的一般性质，从而导致过拟合。因此，需要对决策树进行剪枝从而提升泛化性。

测试泛化性能是否提升的方法：留出法。（留出一部分数据进行验证）

剪枝处理有两种测试思路：
\begin{itemize}
    \item 预剪枝：预剪枝的思路是\textbf{边建树边剪枝}，在划分前先估计该节点的划分是否能提升泛化性能（计算验证集精度），若不能提升精度则不允许划分。

    其优点在于能够降低过拟合风险，减少训练与测试时间开销，但有可能导致欠拟合。

    \item 后剪枝：后剪枝的思路是\textbf{先建树后剪枝}，考察建树之后每个节点替换为叶节点是否能提升验证集精度，可以则剪枝缩点。

    其优点在于能够降低欠拟合风险，泛化性能更强，但其训练时间开销大。
\end{itemize}

\section{多变量决策树}\label{sec:6.4}

单变量决策树的分类边界是与坐标轴平行的，而多变量决策树则是对属性的线性组合，每个节点都是一个形如$\sum_{i=1}^d w_i a_i=t$的线性分类器，其中$ w_i$是属性$a_i$的权值，$w_i$与$t$可在该节点所含的样本集与属性集上学得。

\section{本章往年考试题目}\label{sec:6.5}

\ex{2022年考试原题}{ex_ref}{决策树为何容易过拟合？解决方案是什么？}

详见\ref{sec:6.3}节。

\ex{2023/2025年考试原题}{ex_ref}{决策树最优划分的两个准则，以及他们在属性选择上的偏好是什么？}

详见\ref{sec:6.2}节。
