\chapter{半监督学习}

\section{未标记样本}\label{sec:12.1}
监督学习的成功依赖于大量高质量的标注样本，而实际上许多任务难以获取标注数据，因此需要同时利用有标记样本与未标记样本构建泛化性能良好的模型。

让学习器不依赖外界交互，自动地利用未标记样本来提升学习性能的方法被称为\textbf{半监督学习}。半监督学习对利用的未标记样本做出一系列假设：

\begin{itemize}
    \item 聚类假设：假设数据存在簇结构, 同一簇的样本属于同一类别；
    \item 流形假设：假设数据分布在一个流形结构上, 临近的样本具有相似的输出值。
\end{itemize}

\marginnote{\footnotesize 这两个假设的本质都是“相似的样本拥有相似的输出”。}

\section{生成式方法}\label{sec:12.2}
生成式方法顾名思义，直接基于生成式模型，假设所有数据（无论是否有标记）都由同一个潜在的模型生成，将未标注数据的标记看作潜在模型的缺失参数，通过EM算法极大似然估计求解。

\thm{EM算法求解}{thm_ref}{假设样本$\boldsymbol x$由高斯混合模型生成，每个类别$y\in\mathcal{Y}=\{1,2,\ldots,N\}$对应一个高斯混合成分，则有：\[
p(\boldsymbol{x})=\sum_{i=1}^{k} \alpha_{i} \cdot p\left(\boldsymbol{x} \mid \boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}_{i}\right).
\]其中混合系数$\alpha_i \ge 0, \sum_{i=1}^{n}\alpha_i =1$，$p(\boldsymbol x|\boldsymbol \mu_i, \boldsymbol{\Sigma}_{i})$是样本属于第i个高斯混合成分$(\boldsymbol \mu_i, \boldsymbol{\Sigma}_{i})$的概率。
由最大化后验概率可知：
\[
\begin{array}{l}f(\boldsymbol{x})=\underset{j \in \mathcal{Y}}{\operatorname{argmax}} p(y=j \mid \boldsymbol{x}) \\ =\underset{j \in \mathcal{Y}}{\operatorname{argmax}} \sum_{i=1}^{k} p(y=j, \Theta=i \mid \boldsymbol{x})  \\ =\underset{j \in \mathcal{Y}}{\operatorname{argmax}} \sum_{i=1}^{k} p(y=j \mid \Theta=i, \boldsymbol{x}) \cdot p(\Theta=i \mid \boldsymbol{x})\end{array}
\]其中$ p(\Theta=i \mid x)=\frac{\alpha_{i} p\left(\boldsymbol{x} \mid \boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}_{i}\right)}{\sum_{i=1}^{k} \alpha_{i} p\left(\boldsymbol{x} \mid \boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}_{i}\right)} $代表样本$\boldsymbol x$由第i个高斯混合成分生成的概率（不涉及样本标记），$p(y=j\mid\Theta=i, \boldsymbol x )$为样本$\boldsymbol x$由第i个高斯混合成分生成且类别为j的概率（需知样本标记），由于每个类别对应一个高斯混合成分，可直接代换为$p(y=j\mid\Theta=i)$。

根据有标记样本集$D_l = \{(\boldsymbol x_1, y_1),(\boldsymbol x_2, y_2),\ldots,(\boldsymbol x_l, y_l)\}$与无标记数据集$D_u = \{\boldsymbol x_{l+1}, \boldsymbol x_{l+2}, \ldots，\boldsymbol x_{l+u}\}， l+u= m$，假设所有样本独立同分布且均由同一个高斯混合模型生成，即可通过对数似然估计高斯混合模型的参数$\{(\alpha_i, \boldsymbol \mu_i, \boldsymbol \Sigma_i)\mid 1 \le i \le N \}$：
\[
\begin{aligned} \ln p\left(D_{l} \cup D_{u}\right)= & \sum_{\left(\boldsymbol{x}_{j}, y_{j}\right) \in D_{l}} \ln \left(\sum_{i=1}^{k} \alpha_{i} \cdot p\left(\boldsymbol{x}_{j} \mid \boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}_{i}\right) \cdot p\left(y_{j} \mid \Theta=i, \boldsymbol{x}_{j}\right)\right) \\ & +\sum_{\boldsymbol{x}_{j} \in D_{u}} \ln \left(\sum_{i=1}^{k} \alpha_{i} \cdot p\left(\boldsymbol{x}_{j} \mid \boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}_{i}\right)\right) .\end{aligned}
\]

EM模型求解参数主要分为两步：

E步：根据当前模型参数计算未标记样本$x_j$属于各高斯混合成分的概率：\[
\gamma_{j i}=\frac{\alpha_{i} \cdot p\left(\boldsymbol{x}_{j} \mid \boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}_{i}\right)}{\sum_{i=1}^{N} \alpha_{i} \cdot p\left(\boldsymbol{x}_{j} \mid \boldsymbol{\mu}_{i}, \boldsymbol{\Sigma}_{i})\right.}
\]

M步：基于$\gamma_{ji}$更新模型参数：\[
\boldsymbol{\mu}_{i}=\frac{1}{\sum_{\boldsymbol{x}_{j} \in D_{u}} \gamma_{j i}+l_{i}}\left(\sum_{\boldsymbol{x}_{j} \in D_{u}} \gamma_{j i} \boldsymbol{x}_{j}+\sum_{\left(\boldsymbol{x}_{i}, y_{i}\right) \in D_{l} \wedge y_{j}=i} \boldsymbol{x}_{j}\right)
\]\[
\begin{aligned} \boldsymbol{\Sigma}_{i}=\frac{1}{\sum_{\boldsymbol{x}_{j} \in D_{u}} \gamma_{j i}+l_{i}} & \left(\sum_{\boldsymbol{x}_{j} \in D_{u}} \gamma_{j i}\left(\boldsymbol{x}_{j}-\boldsymbol{\mu}_{i}\right)\left(\boldsymbol{x}_{j}-\boldsymbol{\mu}_{i}\right)^{T}\right. \\ & \left.+\sum_{\left(\boldsymbol{x}_{i}, y_{i}\right) \in D_{l} \wedge y_{j}=i}\left(\boldsymbol{x}_{j}-\boldsymbol{\mu}_{i}\right)\left(\boldsymbol{x}_{j}-\boldsymbol{\mu}_{i}\right)^{T}\right)\end{aligned}
\]\[
\alpha_{i}=\frac{1}{m}\left(\sum_{\boldsymbol{x}_{j} \in D_{u}} \gamma_{j i}+l_{i}\right)
\]
}
将高斯混合模型换成混合专家模型, 朴素贝叶斯模型等, 可推导出其他生成式半监督方法。

生成式方法的优点在于简单，易于实现，在有标记数据极少的情形下往往强于其他方法的性能；缺点是要求模型假设必须准确，否则会影响泛化性能。
\section{半监督SVM}\label{sec:12.3}
半监督支持向量机是支持向量机在半监督学习上的推广，试图找到能将两类有标记样本分开且穿过低密度数据区域的划分超平面。

半监督SVM中最著名的是TSVM，是针对二分类问题的学习方法。TSVM对未标记样本进行各种可能的标记指派，即尝试将每个未标记样本分别作为正例与反例，并在所有这些结果中寻求一个在所有样本上间隔最大化的划分超平面。

形式化地来说，给定有标记样本集$D_l = \{(\boldsymbol x_1, y_1),(\boldsymbol x_2, y_2),\ldots,(\boldsymbol x_l, y_l)\}$与无标记数据集$D_u = \{\boldsymbol x_{l+1}, \boldsymbol x_{l+2}, \ldots，\boldsymbol x_{l+u}\}, y\in\{-1, +1\}, l+u=m$，TSVM的学习目标是为$D_u$中的样本给出预测标记$\hat{\boldsymbol y} = (\hat{y}_{l+1},\hat{y}_{l+2},\ldots,\hat{y}_{l+u})$，使得：
\[
\begin{aligned} \min _{\boldsymbol{w}, b, \hat{\boldsymbol{y}}, \boldsymbol{\xi}} & \frac{1}{2}\|\boldsymbol{w}\|_{2}^{2}+C_{l} \sum_{i=1}^{l} \xi_{i}+C_{u} \sum_{i=l+1}^{m} \xi_{i} \\ \text { s.t. } & y_{i}\left(\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b\right) \geq 1-\xi_{i}, \quad i=1, \ldots, l, \\ & \hat{y}_{i}\left(\boldsymbol{w}^{\top} \boldsymbol{x}_{i}+b\right) \geq 1-\xi_{i}, \quad i=l+1, \ldots, m, \\ & \xi_{i} \geq 0, \quad i=1, \ldots, m,\end{aligned}
\]

TSVM采用局部搜索迭代式地寻找近似解。具体而言：
\begin{itemize}
    \item 先用有标记样本训练一个SVM，为所有无标记样本指派一个伪标记$\hat{y}$并加入数据中进行训练；
    \item 之后设置$C_u \ll C_l$，不断选择标记为异类且容易出错的未标记样本对，交换标记并重新求解超平面$(w,b)$与松弛向量$\xi$，同时逐渐增大$C_u$，直至$C_u = C_l$。
\end{itemize}

半监督SVM可能出现类别不平衡问题，同时搜索标记指派出错的样本对计算开销巨大。
\marginnote{\footnotesize 类别不平衡问题可以将$C_u$拆解为对应正例与负例的$C_u^+, C_u^-$并初始化控制比率来解决。}

\section{图半监督学习}\label{sec:12.4}
将数据集映射为一张图，每个样本对应一个点，若两个样本之间的相似度很高，则对应节点之间存在一条边，边的\textbf{强度}正比于样本之间的\textbf{相似度}。

将所有有标记样本对应的节点想象为“有染色”的节点，半监督学习的过程即对应颜色在图上扩散的过程，由于图与矩阵的对应，我们基于矩阵运算对半监督学习进行推导分析。

\thm{图半监督学习}{thm_ref}{基于有标记数据集与无标记数据集的并$D_l \cup D_u$构建一张图:
\[
G = (V,E), V = \{\boldsymbol x_1,\ldots,\boldsymbol x_l,\boldsymbol x_{l+1},\ldots,\boldsymbol x_{l+u}\}
\]
边集$E$可表示为一个亲和矩阵，基常于高斯函数定义为：\[
\mathbf{W}_{i j}=\left\{\begin{array}{cl}\exp \left(\frac{-\left\|x_{i}-x_{j}\right\|_{2}^{2}}{2 \sigma^{2}}\right), & \text { if } i \neq j \\ 0 & , \text { otherwise }\end{array}\right.
\]
假定从图$G=(V,E)$学的一个实值函数$f:V\to \mathbb{R}$，对应的分类规则为$y_i = \operatorname{sign}(f(\boldsymbol x_i))$，则可定义关于$f$的\textbf{“能量函数”}以最优化学习结果，使得相似的样本拥有相似的标记：
\[
\begin{array}{l}E(f)=\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \mathbf{(W)}_{i j}\left(f\left(\boldsymbol{x}_{i}\right)-f\left(\boldsymbol{x}_{j}\right)\right)^{2} \\ =\frac{1}{2}\left(\sum_{i=1}^{m} d_{i} f^{2}\left(\boldsymbol{x}_{i}\right)+\sum_{j=1}^{m} d_{j} f^{2}\left(\boldsymbol{x}_{j}\right)-2 \sum_{i=1}^{m} \sum_{j=1}^{m} \mathbf{(W)}_{i j} f\left(\boldsymbol{x}_{i}\right) f\left(\boldsymbol{x}_{j}\right)\right) \\ =\boldsymbol{f}^{T}(\mathbf{D}-\mathbf{W}) \boldsymbol{f}\end{array}
\]
其中$\boldsymbol f = (\boldsymbol f_l; \boldsymbol f_u)$分别表示函数$f$在有标记样本与无标记样本上的预测结果，$D$是一个对角矩阵，每一行的对角元素对应矩阵$\mathbf{W}$的第i行元素之和。
将上式转变为分块矩阵的形式，可得：\[
\begin{aligned} E(f) & =\left(f_{l}^{\top} f_{u}^{\top}\right)\left(\left[\begin{array}{cc}\mathbf{D}_{l l} & \mathbf{0}_{l u} \\ \mathbf{0}_{u l} & \mathbf{D}_{u u}\end{array}\right]-\left[\begin{array}{cc}\mathbf{W}_{l l} & \mathbf{W}_{l u} \\ \mathbf{W}_{u l} & \mathbf{W}_{u u}\end{array}\right]\right)\left[\begin{array}{c}f_{l} \\ f_{u}\end{array}\right] \\ & = f_{l}^{\top}\left(\mathbf{D}_{l l}-\mathbf{W}_{l l}\right) f_{l}-2 f_{u}^{\top} \mathbf{W}_{u l} f_{l}+f_{u}^{\top}\left(\mathbf{D}_{u u}-\mathbf{W}_{u u}\right) f_{u} .\end{aligned}
\]
由$ \frac{\partial E(f)}{\partial f_{u}}=\mathbf{0} $可得：
\[ \begin{aligned} \frac{\partial E(f)}{\partial \boldsymbol{f}_{u}} & =\frac{\partial \boldsymbol{f}_{l}^{\mathrm{T}}\left(\boldsymbol{D}_{l l}-\boldsymbol{W}_{l l}\right) \boldsymbol{f}_{l}-2 \boldsymbol{f}_{u}^{\mathrm{T}} \boldsymbol{W}_{u l} \boldsymbol{f}_{l}+\boldsymbol{f}_{u}^{\mathrm{T}}\left(\boldsymbol{D}_{u u}-\boldsymbol{W}_{u u}\right) \boldsymbol{f}_{u}}{\partial \boldsymbol{f}_{u}} \\ & =-2 \boldsymbol{W}_{u l} \boldsymbol{f}_{l}+2\left(\boldsymbol{D}_{u u}-\boldsymbol{W}_{u u}\right) \boldsymbol{f}_{u} = 0 \\
\boldsymbol{f}_{u}&=\left(\mathbf{D}_{u u}-\mathbf{W}_{u u}\right)^{-1} \mathbf{W}_{u l} \boldsymbol{f}_{l}\end{aligned} \]

令\[
\begin{array}{c}\mathbf{P}=\mathbf{D}^{-1} \mathbf{W}=\left[\begin{array}{cc}\mathbf{D}_{l l}^{-1} & 0_{l u} \\ \mathbf{0}_{u l} & \mathbf{D}_{u u}^{-1}\end{array}\right]\left[\begin{array}{cc}\mathbf{W}_{l l} & \mathbf{W}_{l u} \\ \mathbf{W}_{u l} & \mathbf{W}_{u u}\end{array}\right]=\left[\begin{array}{cc}\mathbf{D}_{l l}^{-1} \mathbf{W}_{l l} & \mathbf{D}_{l l}^{-1} \mathbf{W}_{l u} \\ \mathbf{D}_{u u}^{-1} \mathbf{W}_{u l} & \mathbf{D}_{u u}^{-1} \mathbf{W}_{u u}\end{array}\right] \\ \mathbf{P}_{u u}=\mathbf{D}_{u u}^{-1} \mathbf{W}_{u u}, \mathbf{P}_{u l}=\mathbf{D}_{u u}^{-1} \mathbf{W}_{u l}\end{array}，
\]带入前式则有\[
\begin{aligned} \boldsymbol{f}_{u} & =\left(\boldsymbol{D}_{u u}\left(\boldsymbol{I}-\boldsymbol{D}_{u u}^{-1} \boldsymbol{W}_{u u}\right)\right)^{-1} \boldsymbol{W}_{u l} \boldsymbol{f}_{l} \\ & =\left(\boldsymbol{I}-\boldsymbol{D}_{u u}^{-1} \boldsymbol{W}_{u u}\right)^{-1} \boldsymbol{D}_{u u}^{-1} \boldsymbol{W}_{u l} \boldsymbol{f}_{l} \\ & =\left(\boldsymbol{I}-\boldsymbol{P}_{u u}\right)^{-1} \boldsymbol{P}_{u l} \boldsymbol{f}_{l}\end{aligned}
\]
}

\marginnote{\footnotesize 上述描述的是二分类问题标记传播方法，还有适用于多分类的迭代式标记传播方法，但这个不会考的}

图半监督学习的优点是：概念清晰，易于使用矩阵分析探索算法性质；

缺点是：图矩阵的存储开销高，建图时仅考虑了训练样本集，而新样本在图中的位置难以估计，可能需要重构。

\section{基于分歧的方法}\label{sec:12.5}

基于分歧的方法使用多学习器，通过学习器之间的“分歧”来利用未标记数据。

这一类方法的重要代表是协同训练，最初是针对“多视图”数据设计的。在现实应用中，一个数据对象往往同时拥有多个属性集，每个属性集构成一个“视图”。假设不同视图拥有\textbf{相容性}，即其所包含的关于输出空间$\mathcal{Y}$的信息是一致的，协同训练假设数据拥有两个充分（每个视图都包含足以产生最优学习器的信息）且条件独立（给定类别标记调价下两视图独立）的视图，则可通过将每个视图训练出的学习器最确信的样本交给其他学习器、加入下一轮训练的方式提升泛化性。

理论证明：若两个视图充分且条件独立，则可利用未标记样本通过协同训练将弱分类器的泛化性能提升到任意高。但实际测试发现，不一定需要多视图, 只需要弱学习器之间有显著的分歧，也可通过提供伪标记样本的方式提高泛化性能。

这类方法的缺点是: 有标记样本很少时, 不容易生成有显著分歧、性能尚可的多个学习器。

\section{半监督聚类}\label{sec:12.6}
半监督聚类是在聚类上添加额外监督信息，以获得更好的聚类效果的做法。

聚类任务中获得的监督信息大致有两种类型:

第一种类型是“必连”(must-link)与“勿连”(cannot-link)约束，前者是指样本必属于同一个簇,后者则是指样本必不属于同一个簇;

使用这一类监督信息的代表是约束K均值，它在聚类过程中，在确定样本属于的簇时，首先确保必连与勿连关系集合的约束必须满足，若不冲突才选择最近簇，否则尝试次近。

第二种类型的监督信息则是少量的有标记样本。

使用这一类监督信息的代表是约束种子K均值，直接将有标记的样本作为种子初始化K均值算法的K个聚类中心。

\section{本章往年考试题目}\label{sec:12.7}

\ex{2022年考试原题}{ex_ref}{图半监督模型中$E(f) = \boldsymbol{f}^{T}(\mathbf{D}-\mathbf{W}) \boldsymbol{f}$中$E(f)$的含义是什么，D的表达是？}
\ex{2023/2025年考试原题}{ex_ref}{图半监督能量函数的定义是什么？并推导其闭式解。}

$f$是图半监督学习中基于训练数据集生成的图$G=(V,E)$学得的一个实值二分类函数，$E(f) = \boldsymbol{f}^{T}(\mathbf{D}-\mathbf{W}) \boldsymbol{f}$是对函数$f$学习结果的最优化约束，使得相似的样本拥有相似的标记，称为能量函数。

其中$\boldsymbol f = (\boldsymbol f_l; \boldsymbol f_u)$分别表示函数$f$在有标记样本与无标记样本上的预测结果，$D$是一个对角矩阵，每一行的对角元素对应边集E对应的亲和矩阵$\mathbf{W}$的第i行元素之和。

闭式解推导过程详见\ref{sec:12.4}节。



